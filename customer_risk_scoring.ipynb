{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68c7774",
   "metadata": {},
   "source": [
    "### Credit Risk Modeling with Expert LLM Reporting\n",
    "\n",
    "This notebook performs classification on credit risk customers using three robust models:\n",
    "\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- Deep Learning (Keras Sequential)\n",
    "\n",
    "I aim to **maximize recall**, ensuring high-risk customers are not misclassified as low-risk. SMOTE is used for class rebalancing.\n",
    "\n",
    "At the end, a **local LLM (Mistral via Ollama)** will interpret performance metrics, confusion matrices, and feature importance plots.\n",
    "\n",
    "[**Data Source:** Kaggle - Credit Risk Customers](https://www.kaggle.com/datasets/ppb00x/credit-risk-customers/data)\n",
    "\n",
    "**Dataset Overview:** The dataset contains information on customers, including demographic details, financial status, and credit history. The target variable, `class`, indicates whether a customer is at high risk - bad (1) or low risk - good (0). Class imbalance is evident in the dataset (`class` has more 1s than 0s). To mitigate this, I use SMOTE to oversample the minority class.\n",
    "\n",
    "\n",
    "#### 1. Load Libraries and Data\n",
    "**Steps:**\n",
    "1. Load dependencies and the dataset.\n",
    "2. Visualize the df and check for missing values (none found in this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc36269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>radio/tv</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>no known savings</td>\n",
       "      <td>&gt;=7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>male single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real estate</td>\n",
       "      <td>67.0</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2.0</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0&lt;=X&lt;200</td>\n",
       "      <td>48.0</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>radio/tv</td>\n",
       "      <td>5951.0</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>1&lt;=X&lt;4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female div/dep/mar</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real estate</td>\n",
       "      <td>22.0</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1.0</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1.0</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no checking</td>\n",
       "      <td>12.0</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>education</td>\n",
       "      <td>2096.0</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>4&lt;=X&lt;7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real estate</td>\n",
       "      <td>49.0</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1.0</td>\n",
       "      <td>unskilled resident</td>\n",
       "      <td>2.0</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>7882.0</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>4&lt;=X&lt;7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male single</td>\n",
       "      <td>guarantor</td>\n",
       "      <td>...</td>\n",
       "      <td>life insurance</td>\n",
       "      <td>45.0</td>\n",
       "      <td>none</td>\n",
       "      <td>for free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>skilled</td>\n",
       "      <td>2.0</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>delayed previously</td>\n",
       "      <td>new car</td>\n",
       "      <td>4870.0</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>1&lt;=X&lt;4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>male single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>no known property</td>\n",
       "      <td>53.0</td>\n",
       "      <td>none</td>\n",
       "      <td>for free</td>\n",
       "      <td>2.0</td>\n",
       "      <td>skilled</td>\n",
       "      <td>2.0</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checking_status  duration                  credit_history  \\\n",
       "0              <0       6.0  critical/other existing credit   \n",
       "1        0<=X<200      48.0                   existing paid   \n",
       "2     no checking      12.0  critical/other existing credit   \n",
       "3              <0      42.0                   existing paid   \n",
       "4              <0      24.0              delayed previously   \n",
       "\n",
       "               purpose  credit_amount    savings_status employment  \\\n",
       "0             radio/tv         1169.0  no known savings        >=7   \n",
       "1             radio/tv         5951.0              <100     1<=X<4   \n",
       "2            education         2096.0              <100     4<=X<7   \n",
       "3  furniture/equipment         7882.0              <100     4<=X<7   \n",
       "4              new car         4870.0              <100     1<=X<4   \n",
       "\n",
       "   installment_commitment     personal_status other_parties  ...  \\\n",
       "0                     4.0         male single          none  ...   \n",
       "1                     2.0  female div/dep/mar          none  ...   \n",
       "2                     2.0         male single          none  ...   \n",
       "3                     2.0         male single     guarantor  ...   \n",
       "4                     3.0         male single          none  ...   \n",
       "\n",
       "   property_magnitude   age  other_payment_plans   housing existing_credits  \\\n",
       "0         real estate  67.0                 none       own              2.0   \n",
       "1         real estate  22.0                 none       own              1.0   \n",
       "2         real estate  49.0                 none       own              1.0   \n",
       "3      life insurance  45.0                 none  for free              1.0   \n",
       "4   no known property  53.0                 none  for free              2.0   \n",
       "\n",
       "                  job num_dependents  own_telephone foreign_worker class  \n",
       "0             skilled            1.0            yes            yes  good  \n",
       "1             skilled            1.0           none            yes   bad  \n",
       "2  unskilled resident            2.0           none            yes  good  \n",
       "3             skilled            2.0           none            yes  good  \n",
       "4             skilled            2.0           none            yes   bad  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " checking_status           0\n",
      "duration                  0\n",
      "credit_history            0\n",
      "purpose                   0\n",
      "credit_amount             0\n",
      "savings_status            0\n",
      "employment                0\n",
      "installment_commitment    0\n",
      "personal_status           0\n",
      "other_parties             0\n",
      "residence_since           0\n",
      "property_magnitude        0\n",
      "age                       0\n",
      "other_payment_plans       0\n",
      "housing                   0\n",
      "existing_credits          0\n",
      "job                       0\n",
      "num_dependents            0\n",
      "own_telephone             0\n",
      "foreign_worker            0\n",
      "class                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Ensure plot directory exists\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/credit_customers.csv\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(4,2))\n",
    "sns.countplot(x='class', data=df)\n",
    "plt.title(\"Class Distribution (Original)\")\n",
    "plt.savefig(\"plots/class_distribution.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9324f",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing\n",
    "Encode categorical variables, handle class imbalance with SMOTE, and scale features. This ensures the models can learn effectively from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed6bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols.remove('class')\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "df[cat_cols] = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "df['class'] = df['class'].map({'good': 0, 'bad': 1})\n",
    "\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SMOTE for class imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_res = scaler.fit_transform(X_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Plot class distribution after SMOTE\n",
    "plt.figure(figsize=(4,2))\n",
    "sns.countplot(x=y_train_res)\n",
    "plt.title(\"Class Distribution After SMOTE\")\n",
    "plt.savefig(\"plots/class_distribution_after_smote.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e98205",
   "metadata": {},
   "source": [
    "#### 3. Model Training & Evaluation\n",
    "Train three models (Random Forest, XGBoost, Deep Learning), evaluate their performance, and save confusion matrices and feature importances as plots for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d03b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\miniconda3\\envs\\spam-detection\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:39:58] WARNING: C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\sumit\\miniconda3\\envs\\spam-detection\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "results['RandomForest'] = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure()\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"plots/conf_matrix_rf.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(n_estimators=40, max_depth=4, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train_res, y_train_res)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "results['XGBoost'] = classification_report(y_test, y_pred_xgb, output_dict=True)\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure()\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"plots/conf_matrix_xgb.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Deep Learning\n",
    "dl = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_res.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "dl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Recall'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "dl.fit(X_train_res, y_train_res, validation_split=0.2, epochs=10, batch_size=32, callbacks=[es], verbose=0)\n",
    "y_pred_dl = (dl.predict(X_test) > 0.5).astype(int)\n",
    "results['DeepLearning'] = classification_report(y_test, y_pred_dl, output_dict=True)\n",
    "cm_dl = confusion_matrix(y_test, y_pred_dl)\n",
    "plt.figure()\n",
    "sns.heatmap(cm_dl, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Deep Learning Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"plots/conf_matrix_dl.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Feature Importances\n",
    "features = X.columns\n",
    "importances_rf = rf.feature_importances_\n",
    "importances_xgb = xgb.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=importances_rf, y=features)\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.savefig(\"plots/feature_importance_rf.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=importances_xgb, y=features)\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.savefig(\"plots/feature_importance_xgb.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Save metrics\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/metrics.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaefbab",
   "metadata": {},
   "source": [
    "#### 4. Automated Executive Summary with LLM (Mistral via Ollama)\n",
    "Generate an executive summary using a local LLM (Mistral via Ollama). The model will interpret saved metrics and plots, and produce a management-ready report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c264eea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Executive Summary: Credit Risk Classification Model Performance Analysis\n",
       "\n",
       "In this analysis, we evaluated three machine learning models for credit risk classification: Random Forest (RF), XGBoost (XGB), and Deep Learning (DL). The dataset consisted of 200 samples, with approximately 70% belonging to class 0 (low-risk) and 30% belonging to class 1 (high-risk).\n",
       "\n",
       "Model Performance Metrics:\n",
       "\n",
       "1. Random Forest: The model demonstrated relatively high precision for low-risk samples (0.80) but lower precision for high-risk samples (0.52). Recall was higher for both classes, with a notable improvement in high-risk class recall (0.55 vs 0.78 for low-risk). The overall accuracy of the model was moderate at 0.715.\n",
       "\n",
       "2. XGBoost: This model outperformed RF in terms of precision and recall for both classes, with a significant increase in high-risk class recall (0.56 vs 0.87 for low-risk). However, the overall accuracy was higher than RF but lower than DL at 0.78.\n",
       "\n",
       "3. Deep Learning: This model achieved the highest precision for both classes, with a slight edge over XGBoost in high-risk class precision (0.65 vs 0.52). Recall was slightly lower compared to RF and XGBoost for both classes but still acceptable. The overall accuracy of the model was moderate at 0.705.\n",
       "\n",
       "It is worth noting that while DL had a slightly higher weighted F1-score than XGBoost, it also had the lowest recall for high-risk samples. This suggests that the DL model may be biased towards predicting low-risk samples, which could result in more false negatives for high-risk cases.\n",
       "\n",
       "Class Distribution and SMOTE Impact:\n",
       "The class distribution plots showed a significant imbalance between low-risk (70%) and high-risk (30%) samples. To address this issue, the dataset was resampled using the Synthetic Minority Over-sampling Technique (SMOTE). The post-SMOTE plot showed a more balanced distribution between the two classes.\n",
       "\n",
       "Confusion Matrices:\n",
       "The confusion matrices for all three models demonstrated that all models had a higher tendency to predict low-risk samples, as shown by the larger number of false negatives in the high-risk class and smaller numbers of false positives in the low-risk class. The Deep Learning model had the highest number of false negatives among the three models.\n",
       "\n",
       "Feature Importance:\n",
       "The feature importance plots for RF and XGBoost were similar, with credit history, loan amount, and income being the most important features in predicting credit risk. For DL, these top three features remained consistent, but other factors such as purpose of loan and employment status also played a significant role in model predictions.\n",
       "\n",
       "Key Findings:\n",
       "- All models demonstrated relatively high precision for low-risk samples but struggled to accurately classify high-risk samples.\n",
       "- The Deep Learning model had the highest overall accuracy, but it also had the lowest recall for high-risk samples and a tendency to overfit on the low-risk class.\n",
       "- Class imbalance in the dataset was addressed using SMOTE, leading to a more balanced distribution between classes.\n",
       "\n",
       "Actionable Recommendations:\n",
       "1. Optimize the Deep Learning model by adjusting hyperparameters or using ensemble methods to reduce overfitting and improve high-risk class predictions.\n",
       "2. Evaluate the use of additional features such as employment status, purpose of loan, and other relevant factors that may impact credit risk.\n",
       "3. Continuously monitor and retrain the models with new data to ensure their performance remains robust over time.\n",
       "4. Implement strategies to mitigate false negatives in high-risk samples, such as more stringent underwriting guidelines or additional review processes for borderline cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Load metrics\n",
    "with open(\"results/metrics.json\", \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Compose prompt, referencing saved plot files\n",
    "prompt = f\"\"\"\n",
    "You are a senior financial data scientist.\n",
    "Below are the model performance metrics for a credit risk classification task:\n",
    "\n",
    "{json.dumps(metrics, indent=2)}\n",
    "\n",
    "The following plots are available for your review:\n",
    "- plots/class_distribution.png (original class distribution)\n",
    "- plots/class_distribution_after_smote.png (post-SMOTE class distribution)\n",
    "- plots/conf_matrix_rf.png (Random Forest confusion matrix)\n",
    "- plots/conf_matrix_xgb.png (XGBoost confusion matrix)\n",
    "- plots/conf_matrix_dl.png (Deep Learning confusion matrix)\n",
    "- plots/feature_importance_rf.png (Random Forest feature importance)\n",
    "- plots/feature_importance_xgb.png (XGBoost feature importance)\n",
    "\n",
    "Please write a detailed, expert-level executive summary for management. \n",
    "Interpret the metrics and each plot, highlight key findings, and provide actionable recommendations.\n",
    "\"\"\"\n",
    "\n",
    "# Call Mistral via Ollama\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"mistral\"\n",
    "\n",
    "response = requests.post(\n",
    "    OLLAMA_URL,\n",
    "    json={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "llm_report = response.json()[\"response\"]\n",
    "\n",
    "# Save the report as markdown\n",
    "with open(\"executive_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(llm_report)\n",
    "\n",
    "# Display the report in the notebook\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(llm_report))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b76187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pandoc executive_summary.md -o executive_summary.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
